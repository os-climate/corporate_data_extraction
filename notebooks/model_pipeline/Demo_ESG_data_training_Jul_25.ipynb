{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from esg_data_pipeline.components import FARMTrainer\n",
    "from esg_data_pipeline.config import (ModelConfig, \n",
    "                                      TokenizerConfig, \n",
    "                                      TrainingConfig, \n",
    "                                      FileConfig, \n",
    "                                      MLFlowConfig,\n",
    "                                      ProcessorConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training pipeline trains the relevance classifier once the dataset has been prepared and curated. The model trained comprises a transformers model such as BERT which can be loaded pre-trained into the pipeline and then be fine-tuned on the curated data for our specific relevance detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline includes components provided by the FARM library. FARM is a framework which facilates transfer learning tasks for BERT based model. Documentation is available here: https://farm.deepset.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our demo we use the curated data generated after receiving the last set of annotations from Allianz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training, parameters for each component of the training pipeline must be set. For this we create `config` objects which hold these parameters. Default values have already been set but they can be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = FileConfig() #Settings data files and checkpoints parameters\n",
    "processor_config = ProcessorConfig() #Settings for the processor component\n",
    "tokenizer_config = TokenizerConfig() #Settings for the tokenizer\n",
    "model_config = ModelConfig() #Settings for the model\n",
    "train_config = TrainingConfig() #Settings for training\n",
    "mlflow_config = MLFlowConfig() #Settings for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be changed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config.experiment_name = \"demo_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we advise to manually update the parameters in corresponding config file: `esg_data_pipeline/config/config_farm_trainer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the value for some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Experiment_name: \\n {file_config.experiment_name} \\n\")\n",
    "print(f\"Curated dataset path: \\n {file_config.curated_data} \\n\")\n",
    "print(f\"Split train/validation ratio: \\n{file_config.test_split} \\n\")\n",
    "print(f\"Training dataset path: \\n {file_config.train_filename} \\n\")\n",
    "print(f\"Validation dataset path: \\n {file_config.dev_filename} \\n\")\n",
    "print(f\"Directory where trained model is saved: \\n {file_config.saved_models_dir} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max number of tokens per example: {processor_config.max_seq_len} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Use GPU: {train_config.use_cuda} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Learning_rate: {train_config.learning_rate} \\n\")\n",
    "print(f\"Number of epochs for fine tuning: {train_config.n_epochs} \\n\")\n",
    "print(f\"Batch size: {train_config.batch_size} \\n\")\n",
    "print(f\"Perform Cross validation: {train_config.run_cv} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model trained on NQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already trained a relevance classifier on Google's large NQ dataset. We then saved the model in the following directory: `file_config.saved_models_dir / \"relevance_roberta\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load this model in our pipeline to fine-tune a relevance classifier on our specific ESG curated dataset. For this we have to set the parameter `model_config.load_dir` to be the directory where we saved our first checkpoint. We can check that this is set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NQ checkpoint directory: {model_config.load_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune on curated ESG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the parameters are set a `FARMTrainer` object can be instantiated by passing all the configuration objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_trainer = FARMTrainer(\n",
    "        file_config =file_config,\n",
    "        tokenizer_config=tokenizer_config,\n",
    "        model_config=model_config,\n",
    "        processor_config=processor_config,\n",
    "        training_config=train_config,\n",
    "        mlflow_config=mlflow_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method `run()` to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
