{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 04:56:32 - INFO - transformers.file_utils -   PyTorch version 1.5.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from model_pipeline import FARMTrainer\n",
    "from model_pipeline import (ModelConfig, \n",
    "                              TokenizerConfig, \n",
    "                              TrainingConfig, \n",
    "                              FileConfig, \n",
    "                              MLFlowConfig,\n",
    "                              ProcessorConfig,\n",
    "                              InferConfig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training pipeline trains the relevance classifier once the dataset has been extracted and curated. The model trained is comprised of a transformer model (e.g., BERT) that can be loaded pre-trained on the NQ dataset into the pipeline and then be fine-tuned on the curated data for our specific relevance detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline includes components that are provided by the FARM library. FARM is a framework which facilitates transfer learning tasks for BERT based models. Documentation for FARM is available here: https://farm.deepset.ai.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training, parameters for each component of the training pipeline must be set. For this we create `config` objects which hold these parameters. Default values have already been set but they can be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = FileConfig() #Settings data files and checkpoints parameters\n",
    "processor_config = ProcessorConfig() #Settings for the processor component\n",
    "tokenizer_config = TokenizerConfig() #Settings for the tokenizer\n",
    "model_config = ModelConfig() #Settings for the model\n",
    "train_config = TrainingConfig() #Settings for training\n",
    "mlflow_config = MLFlowConfig() #Settings for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be changed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config.experiment_name = \"demo_training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we advise that you manually update the parameters in the corresponding config file:\n",
    "\n",
    "`esg_data_pipeline/config/config_farm_trainer.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the value for some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment_name: \n",
      " demo_training \n",
      "\n",
      "Data directory: \n",
      " /model_pipeline/model_pipeline/data \n",
      "\n",
      "Curated dataset path: \n",
      " /model_pipeline/model_pipeline/data/curation/esg_TEXT_dataset.csv \n",
      "\n",
      "Split train/validation ratio: \n",
      "0.2 \n",
      "\n",
      "Training dataset path: \n",
      " /model_pipeline/model_pipeline/data/train_split_02.csv \n",
      "\n",
      "Validation dataset path: \n",
      " /model_pipeline/model_pipeline/data/val_split_02.csv \n",
      "\n",
      "Directory where trained model is saved: \n",
      " /model_pipeline/model_pipeline/saved_models/test_farm \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Experiment_name: \\n {file_config.experiment_name} \\n\")\n",
    "print(f\"Data directory: \\n {file_config.data_dir} \\n\")\n",
    "print(f\"Curated dataset path: \\n {file_config.curated_data} \\n\")\n",
    "print(f\"Split train/validation ratio: \\n{file_config.dev_split} \\n\")\n",
    "print(f\"Training dataset path: \\n {file_config.train_filename} \\n\")\n",
    "print(f\"Validation dataset path: \\n {file_config.dev_filename} \\n\")\n",
    "print(f\"Directory where trained model is saved: \\n {file_config.saved_models_dir} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens per example: 512 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max number of tokens per example: {processor_config.max_seq_len} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Use GPU: {train_config.use_cuda} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning_rate: 1.2168533249479066e-05 \n",
      "\n",
      "Number of epochs for fine tuning: 1 \n",
      "\n",
      "Batch size: 4 \n",
      "\n",
      "Perform Cross validation: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Learning_rate: {train_config.learning_rate} \\n\")\n",
    "print(f\"Number of epochs for fine tuning: {train_config.n_epochs} \\n\")\n",
    "print(f\"Batch size: {train_config.batch_size} \\n\")\n",
    "print(f\"Perform Cross validation: {train_config.run_cv} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table vs. Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model architecture is used to tain for both table and text data, although the final trained models for the two data types will be different. We thus need to train the model two times, once for text data and another time for table data.\n",
    "In order to switch between the two data types, the parameter `data_type` in the config file must be set to either `Text` or `Table`, as shown in the following cell. This will enable the appropriate pre-processing component of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: \n",
      " Text \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_config.data_type = \"Text\"\n",
    "print(f\"Data type: \\n {file_config.data_type} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model trained on NQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already trained a relevance classifier on Google's large NQ dataset. We then saved the model in the following directory: `file_config.saved_models_dir / \"relevance_roberta\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load this model in our pipeline to fine-tune a relevance classifier on our specific ESG curated dataset. For this we have to set the parameter `model_config.load_dir` to be the directory where we saved our first checkpoint. We can check that this is set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NQ checkpoint directory: /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta\n"
     ]
    }
   ],
   "source": [
    "print(f\"NQ checkpoint directory: {model_config.load_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune on curated ESG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the parameters are set a `FARMTrainer` object can be instantiated by passing all the configuration objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_trainer = FARMTrainer(\n",
    "        file_config =file_config,\n",
    "        tokenizer_config=tokenizer_config,\n",
    "        model_config=model_config,\n",
    "        processor_config=processor_config,\n",
    "        training_config=train_config,\n",
    "        mlflow_config=mlflow_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method `run()` to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 04:56:45 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: True\n",
      "08/26/2020 04:56:45 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'RobertaTokenizer'\n",
      "08/26/2020 04:56:46 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "08/26/2020 04:56:46 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "2020/08/26 04:56:46 WARNING mlflow.tracking.context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.data_silo -   Loading train set from: /model_pipeline/model_pipeline/data/train_split_02.csv \n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.data_silo -   Got ya 15 parallel workers to convert 665 dictionaries to pytorch datasets (chunksize = 9)...\n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.data_silo -   /|\\  /w\\  /|\\  /|\\  /w\\  /|\\  /|\\  /w\\  /|\\  /|\\  /|\\  /w\\  /w\\  /w\\  /w\\\n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.data_silo -   /'\\  / \\  /'\\  /'\\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  / \\  /'\\  /'\\\n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.data_silo -                               \n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/train_split_02.csv:   0%|          | 0/665 [00:00<?, ? Dicts/s]08/26/2020 04:56:46 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/26/2020 04:56:46 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 6-0\n",
      "Clear Text: \n",
      " \ttext: What is the total amount of upstream energy indirect greenhouse gases emissions referred to as scope 3 emissions?\n",
      " \ttext_b: In 2018, our Scope 3 emissions were around 113 mn t CO2 equivalent (2017: 108 mn t CO2 equivalent) and are related to total product sales volumes, as well as purchased goods and services and capital goods of all our fully consolidated companies.\n",
      " \ttext_classification_label: 1\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġamount', 'Ġof', 'Ġupstream', 'Ġenergy', 'Ġindirect', 'Ġgreenhouse', 'Ġgases', 'Ġemissions', 'Ġreferred', 'Ġto', 'Ġas', 'Ġscope', 'Ġ3', 'Ġemissions', '?']\n",
      " \ttokens_b: ['In', 'Ġ2018', ',', 'Ġour', 'ĠScope', 'Ġ3', 'Ġemissions', 'Ġwere', 'Ġaround', 'Ġ113', 'Ġm', 'n', 'Ġt', 'ĠCO', '2', 'Ġequivalent', 'Ġ(', '2017', ':', 'Ġ108', 'Ġm', 'n', 'Ġt', 'ĠCO', '2', 'Ġequivalent', ')', 'Ġand', 'Ġare', 'Ġrelated', 'Ġto', 'Ġtotal', 'Ġproduct', 'Ġsales', 'Ġvolumes', ',', 'Ġas', 'Ġwell', 'Ġas', 'Ġpurchased', 'Ġgoods', 'Ġand', 'Ġservices', 'Ġand', 'Ġcapital', 'Ġgoods', 'Ġof', 'Ġall', 'Ġour', 'Ġfully', 'Ġconsolidated', 'Ġcompanies', '.']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 1280, 9, 21561, 1007, 18677, 11832, 20038, 5035, 4997, 7, 25, 7401, 155, 5035, 116, 2, 2, 1121, 199, 6, 84, 30108, 155, 5035, 58, 198, 14881, 475, 282, 326, 6247, 176, 6305, 36, 3789, 35, 13955, 475, 282, 326, 6247, 176, 6305, 43, 8, 32, 1330, 7, 746, 1152, 647, 7267, 6, 25, 157, 25, 3584, 3057, 8, 518, 8, 812, 3057, 9, 70, 84, 1950, 14436, 451, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 04:56:47 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 3-0\n",
      "Clear Text: \n",
      " \ttext: What is the total volume of natural gas liquid production?\n",
      " \ttext_b: During the past year, NOVATEK’s hydrocarbon production totaled 549.1 million boe, including 68.8 bcm of natural gas and 11,800 thousand tons of liquids\n",
      " \ttext_classification_label: 1\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġvolume', 'Ġof', 'Ġnatural', 'Ġgas', 'Ġliquid', 'Ġproduction', '?']\n",
      " \ttokens_b: ['During', 'Ġthe', 'Ġpast', 'Ġyear', ',', 'ĠNO', 'V', 'ATE', 'K', 'âĢ', 'Ļ', 's', 'Ġhydro', 'carbon', 'Ġproduction', 'Ġtotaled', 'Ġ5', '49', '.', '1', 'Ġmillion', 'Ġbo', 'e', ',', 'Ġincluding', 'Ġ68', '.', '8', 'Ġb', 'cm', 'Ġof', 'Ġnatural', 'Ġgas', 'Ġand', 'Ġ11', ',', '800', 'Ġthousand', 'Ġtons', 'Ġof', 'Ġliquids']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 3149, 9, 1632, 1123, 6936, 931, 116, 2, 2, 14229, 5, 375, 76, 6, 8228, 846, 8625, 530, 17, 27, 29, 13575, 23612, 931, 15137, 195, 3414, 4, 134, 153, 5276, 242, 6, 217, 5595, 4, 398, 741, 13753, 9, 1632, 1123, 8, 365, 6, 3913, 7673, 7741, 9, 26140, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/train_split_02.csv: 100%|██████████| 665/665 [00:05<00:00, 114.34 Dicts/s]\n",
      "08/26/2020 04:56:52 - INFO - farm.data_handler.data_silo -   Loading dev set from: /model_pipeline/model_pipeline/data/val_split_02.csv\n",
      "08/26/2020 04:56:52 - INFO - farm.data_handler.data_silo -   Got ya 15 parallel workers to convert 167 dictionaries to pytorch datasets (chunksize = 4)...\n",
      "08/26/2020 04:56:52 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n",
      "08/26/2020 04:56:52 - INFO - farm.data_handler.data_silo -   /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /|\\  /|\\  /|\\\n",
      "08/26/2020 04:56:52 - INFO - farm.data_handler.data_silo -   / \\  /'\\  / \\  /'\\  / \\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\  /'\\\n",
      "08/26/2020 04:56:52 - INFO - farm.data_handler.data_silo -                               \n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/val_split_02.csv:   0%|          | 0/167 [00:00<?, ? Dicts/s]08/26/2020 04:56:53 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/26/2020 04:56:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 3-0\n",
      "Clear Text: \n",
      " \ttext: What is the base year for carbon reduction commitment?\n",
      " \ttext_b: ENDESA has publicly undertaken to contribute specifically to the achievement of 3 of the 17 Sustainable Development Goals (SDG), while contributing to these public commitments of the ENEL Group through the different projects on which it is engaged. Since 2016, it has also laid out a roadmap to contribute specifically to the following goals: SDG 13 (Climate action): Decarbonisation of the energy mix by 2050, setting intermediate targets to reduce absolute emissions of carbon dioxide (CO2) by 47% in 2020, 61% in 2030, 80% in 2040 and 100% in 2050 with respect to 2005.\n",
      " \ttext_classification_label: 1\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġbase', 'Ġyear', 'Ġfor', 'Ġcarbon', 'Ġreduction', 'Ġcommitment', '?']\n",
      " \ttokens_b: ['END', 'ESA', 'Ġhas', 'Ġpublicly', 'Ġundertaken', 'Ġto', 'Ġcontribute', 'Ġspecifically', 'Ġto', 'Ġthe', 'Ġachievement', 'Ġof', 'Ġ3', 'Ġof', 'Ġthe', 'Ġ17', 'ĠSustainable', 'ĠDevelopment', 'ĠGoals', 'Ġ(', 'SD', 'G', '),', 'Ġwhile', 'Ġcontributing', 'Ġto', 'Ġthese', 'Ġpublic', 'Ġcommitments', 'Ġof', 'Ġthe', 'ĠEN', 'EL', 'ĠGroup', 'Ġthrough', 'Ġthe', 'Ġdifferent', 'Ġprojects', 'Ġon', 'Ġwhich', 'Ġit', 'Ġis', 'Ġengaged', '.', 'ĠSince', 'Ġ2016', ',', 'Ġit', 'Ġhas', 'Ġalso', 'Ġlaid', 'Ġout', 'Ġa', 'Ġroadmap', 'Ġto', 'Ġcontribute', 'Ġspecifically', 'Ġto', 'Ġthe', 'Ġfollowing', 'Ġgoals', ':', 'ĠSD', 'G', 'Ġ13', 'Ġ(', 'Climate', 'Ġaction', '):', 'ĠDec', 'arbon', 'isation', 'Ġof', 'Ġthe', 'Ġenergy', 'Ġmix', 'Ġby', 'Ġ2050', ',', 'Ġsetting', 'Ġintermediate', 'Ġtargets', 'Ġto', 'Ġreduce', 'Ġabsolute', 'Ġemissions', 'Ġof', 'Ġcarbon', 'Ġdioxide', 'Ġ(', 'CO', '2', ')', 'Ġby', 'Ġ47', '%', 'Ġin', 'Ġ2020', ',', 'Ġ61', '%', 'Ġin', 'Ġ2030', ',', 'Ġ80', '%', 'Ġin', 'Ġ20', '40', 'Ġand', 'Ġ100', '%', 'Ġin', 'Ġ2050', 'Ġwith', 'Ġrespect', 'Ġto', 'Ġ2005', '.']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 1542, 76, 13, 4363, 4878, 2720, 116, 2, 2, 9309, 32416, 34, 3271, 15050, 7, 5042, 4010, 7, 5, 8312, 9, 155, 9, 5, 601, 20586, 2717, 20638, 36, 6243, 534, 238, 150, 8216, 7, 209, 285, 9116, 9, 5, 13245, 3721, 826, 149, 5, 430, 1377, 15, 61, 24, 16, 4009, 4, 1773, 336, 6, 24, 34, 67, 4976, 66, 10, 25291, 7, 5042, 4010, 7, 5, 511, 1175, 35, 12723, 534, 508, 36, 40466, 814, 3256, 1502, 40035, 3258, 9, 5, 1007, 3344, 30, 24050, 6, 2749, 21398, 3247, 7, 1888, 7833, 5035, 9, 4363, 15746, 36, 6335, 176, 43, 30, 4034, 207, 11, 2760, 6, 5659, 207, 11, 12060, 6, 1812, 207, 11, 291, 1749, 8, 727, 207, 11, 24050, 19, 2098, 7, 4013, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 04:56:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: What is the target year for climate commitment?\n",
      " \ttext_b: we became the first electric utility in the country to announce our aspiration to produce 100-percent carbon-free electricity for customers by 2050.\n",
      " \ttext_classification_label: 1\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtarget', 'Ġyear', 'Ġfor', 'Ġclimate', 'Ġcommitment', '?']\n",
      " \ttokens_b: ['we', 'Ġbecame', 'Ġthe', 'Ġfirst', 'Ġelectric', 'Ġutility', 'Ġin', 'Ġthe', 'Ġcountry', 'Ġto', 'Ġannounce', 'Ġour', 'Ġaspiration', 'Ġto', 'Ġproduce', 'Ġ100', '-', 'percent', 'Ġcarbon', '-', 'free', 'Ġelectricity', 'Ġfor', 'Ġcustomers', 'Ġby', 'Ġ2050', '.']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 1002, 76, 13, 2147, 2720, 116, 2, 2, 1694, 1059, 5, 78, 3459, 6041, 11, 5, 247, 7, 4659, 84, 34102, 7, 2592, 727, 12, 13566, 4363, 12, 3743, 4382, 13, 916, 30, 24050, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/val_split_02.csv: 100%|██████████| 167/167 [00:03<00:00, 51.88 Dicts/s]\n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   No test set is being loaded\n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   Examples in train: 665\n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   Examples in dev  : 167\n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   Examples in test : 0\n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   \n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     255\n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 68.5624060150376\n",
      "08/26/2020 04:56:56 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.0\n",
      "08/26/2020 04:56:56 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "08/26/2020 04:56:56 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "08/26/2020 04:57:02 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing RobertaModel.\n",
      "\n",
      "08/26/2020 04:57:02 - INFO - transformers.modeling_utils -   All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "08/26/2020 04:57:02 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "08/26/2020 04:57:04 - INFO - transformers.modeling_utils -   loading weights file /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta/language_model.bin from cache at /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta/language_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 04:57:09 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing RobertaModel.\n",
      "\n",
      "08/26/2020 04:57:09 - INFO - transformers.modeling_utils -   All the weights of RobertaModel were initialized from the model checkpoint at /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta/language_model.bin.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "08/26/2020 04:57:09 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "08/26/2020 04:57:09 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "08/26/2020 04:57:09 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "08/26/2020 04:57:09 - INFO - farm.modeling.prediction_head -   Loading prediction head from /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta/prediction_head_0.bin\n",
      "08/26/2020 04:57:10 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1.2168533249479066e-05}'\n",
      "08/26/2020 04:57:10 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "08/26/2020 04:57:10 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 16.7, 'num_training_steps': 167}'\n",
      "08/26/2020 04:57:10 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):  18%|█▊        | 30/167 [00:08<00:35,  3.89it/s]\n",
      "Evaluating: 100%|██████████| 42/42 [00:03<00:00, 13.97it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "08/26/2020 04:57:21 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 30 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/26/2020 04:57:21 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/26/2020 04:57:21 - INFO - farm.eval -   loss: 2.8053443588896427e-05\n",
      "08/26/2020 04:57:21 - INFO - farm.eval -   task_name: text_classification\n",
      "08/26/2020 04:57:21 - INFO - farm.eval -   acc: 1.0\n",
      "08/26/2020 04:57:21 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    1.0000    1.0000       167\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000       167\n",
      "   macro avg     0.5000    0.5000    0.5000       167\n",
      "weighted avg     1.0000    1.0000    1.0000       167\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):  36%|███▌      | 60/167 [00:18<00:26,  3.99it/s]\n",
      "Evaluating: 100%|██████████| 42/42 [00:03<00:00, 13.97it/s]\n",
      "08/26/2020 04:57:32 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 60 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/26/2020 04:57:32 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/26/2020 04:57:32 - INFO - farm.eval -   loss: 1.1281338994374532e-05\n",
      "08/26/2020 04:57:32 - INFO - farm.eval -   task_name: text_classification\n",
      "08/26/2020 04:57:32 - INFO - farm.eval -   acc: 1.0\n",
      "08/26/2020 04:57:32 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    1.0000    1.0000       167\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000       167\n",
      "   macro avg     0.5000    0.5000    0.5000       167\n",
      "weighted avg     1.0000    1.0000    1.0000       167\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):  54%|█████▍    | 90/167 [00:29<00:19,  3.95it/s]\n",
      "Evaluating: 100%|██████████| 42/42 [00:03<00:00, 13.98it/s]\n",
      "08/26/2020 04:57:43 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 90 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/26/2020 04:57:43 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/26/2020 04:57:43 - INFO - farm.eval -   loss: 9.356858487614615e-06\n",
      "08/26/2020 04:57:43 - INFO - farm.eval -   task_name: text_classification\n",
      "08/26/2020 04:57:43 - INFO - farm.eval -   acc: 1.0\n",
      "08/26/2020 04:57:43 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    1.0000    1.0000       167\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000       167\n",
      "   macro avg     0.5000    0.5000    0.5000       167\n",
      "weighted avg     1.0000    1.0000    1.0000       167\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):  72%|███████▏  | 120/167 [00:40<00:11,  4.01it/s]\n",
      "Evaluating: 100%|██████████| 42/42 [00:03<00:00, 13.98it/s]\n",
      "08/26/2020 04:57:53 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 120 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/26/2020 04:57:53 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/26/2020 04:57:53 - INFO - farm.eval -   loss: 8.620187907875655e-06\n",
      "08/26/2020 04:57:53 - INFO - farm.eval -   task_name: text_classification\n",
      "08/26/2020 04:57:53 - INFO - farm.eval -   acc: 1.0\n",
      "08/26/2020 04:57:53 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    1.0000    1.0000       167\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000       167\n",
      "   macro avg     0.5000    0.5000    0.5000       167\n",
      "weighted avg     1.0000    1.0000    1.0000       167\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000):  90%|████████▉ | 150/167 [00:50<00:04,  4.02it/s]\n",
      "Evaluating: 100%|██████████| 42/42 [00:03<00:00, 13.98it/s]\n",
      "08/26/2020 04:58:04 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 150 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 04:58:04 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/26/2020 04:58:04 - INFO - farm.eval -   loss: 8.346077924716973e-06\n",
      "08/26/2020 04:58:04 - INFO - farm.eval -   task_name: text_classification\n",
      "08/26/2020 04:58:04 - INFO - farm.eval -   acc: 1.0\n",
      "08/26/2020 04:58:04 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    1.0000    1.0000       167\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000       167\n",
      "   macro avg     0.5000    0.5000    0.5000       167\n",
      "weighted avg     1.0000    1.0000    1.0000       167\n",
      "\n",
      "Train epoch 0/0 (Cur. train loss: 0.0000): 100%|██████████| 167/167 [00:58<00:00,  2.88it/s]\n",
      "Evaluating: 100%|██████████| 42/42 [00:03<00:00, 13.96it/s]\n",
      "08/26/2020 04:58:11 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 42 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/26/2020 04:58:11 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/26/2020 04:58:11 - INFO - farm.eval -   loss: 8.263273867304454e-06\n",
      "08/26/2020 04:58:11 - INFO - farm.eval -   task_name: text_classification\n",
      "08/26/2020 04:58:11 - INFO - farm.eval -   acc: 1.0\n",
      "08/26/2020 04:58:11 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    1.0000    1.0000       167\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000       167\n",
      "   macro avg     0.5000    0.5000    0.5000       167\n",
      "weighted avg     1.0000    1.0000    1.0000       167\n",
      "\n",
      "08/26/2020 04:58:12 - INFO - model_pipeline.farm_trainer -   Trained model saved to /model_pipeline/model_pipeline/saved_models/test_farm\n",
      "08/26/2020 04:58:12 - INFO - model_pipeline.farm_trainer -   Processor vocabulary saved to /model_pipeline/model_pipeline/saved_models/test_farm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farm_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the training process, the model and the processor vocabulary are saved into the directory `file_config.saved_models_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 488312\r\n",
      "drwxr-xr-x 2 root root      4096 Aug 26 04:58 .\r\n",
      "drwxr-xr-x 7 root root      4096 Aug 26 04:58 ..\r\n",
      "-rw-r--r-- 1 root root 498630327 Aug 26 04:58 language_model.bin\r\n",
      "-rw-r--r-- 1 root root       562 Aug 26 04:58 language_model_config.json\r\n",
      "-rw-r--r-- 1 root root    456318 Aug 26 04:58 merges.txt\r\n",
      "-rw-r--r-- 1 root root      6879 Aug 26 04:58 prediction_head_0.bin\r\n",
      "-rw-r--r-- 1 root root       556 Aug 26 04:58 prediction_head_0_config.json\r\n",
      "-rw-r--r-- 1 root root       727 Aug 26 04:58 processor_config.json\r\n",
      "-rw-r--r-- 1 root root       772 Aug 26 04:58 special_tokens_map.json\r\n",
      "-rw-r--r-- 1 root root       189 Aug 26 04:58 tokenizer_config.json\r\n",
      "-rw-r--r-- 1 root root    898822 Aug 26 04:58 vocab.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al $file_config.saved_models_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better estimate the performance of the model on new data, it is recommended to perform k-folds cross validation (CV). CV works as follows:\n",
    "\n",
    "- Split the entire data randomly into k folds (usually 5 to 10)\n",
    "- Fit the model using the K — 1 folds and validate the model using the remaining Kth fold and save the scores\n",
    "- Repeat until every K-fold serve as the test set and average the saved scores\n",
    "\n",
    "_FARMTrainer_ includes this features. To perform 3-fold CV proceed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config.run_cv = True\n",
    "train_config.xval_folds = 3\n",
    "train_config.n_epochs =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_trainer = FARMTrainer(\n",
    "        file_config =file_config,\n",
    "        tokenizer_config=tokenizer_config,\n",
    "        model_config=model_config,\n",
    "        processor_config=processor_config,\n",
    "        training_config=train_config,\n",
    "        mlflow_config=mlflow_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "farm_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! CV mode does not save a checkpoint, it is only used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the saved model and test it on some real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pathlib\n",
    "\n",
    "from model_pipeline.relevance_infer import TextRelevanceInfer\n",
    "from model_pipeline.config_farm_train import InferConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_config = InferConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will load the model trained by 1QBit. Skip it if you want to use your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneqbit_checkpoint_dir = pathlib.Path(\"/model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG\")\n",
    "infer_config.load_dir = {\"Text\": oneqbit_checkpoint_dir / \"esg_text_checkpoint\",\n",
    "                \"Table\": oneqbit_checkpoint_dir / \"esg_table_checkpoint\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 05:10:18 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "08/26/2020 05:10:18 - INFO - transformers.modeling_utils -   loading weights file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/language_model.bin from cache at /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/language_model.bin\n",
      "08/26/2020 05:10:23 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing RobertaModel.\n",
      "\n",
      "08/26/2020 05:10:23 - INFO - transformers.modeling_utils -   All the weights of RobertaModel were initialized from the model checkpoint at /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/language_model.bin.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "08/26/2020 05:10:23 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "08/26/2020 05:10:23 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "08/26/2020 05:10:23 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "08/26/2020 05:10:23 - INFO - farm.modeling.prediction_head -   Loading prediction head from /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/prediction_head_0.bin\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   Model name '/model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   Didn't find file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/added_tokens.json. We won't load it.\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   Didn't find file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/tokenizer.json. We won't load it.\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/vocab.json\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/merges.txt\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/special_tokens_map.json\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_text_checkpoint/tokenizer_config.json\n",
      "08/26/2020 05:10:23 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "08/26/2020 05:10:23 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "08/26/2020 05:10:23 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "component = TextRelevanceInfer(infer_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on a Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/26/2020 05:11:17 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/26/2020 05:11:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: Is the company going to go green?\n",
      " \ttext_b: The company is going to reduce 8% in gas production\n",
      "Tokenized: \n",
      " \ttokens: ['Is', 'Ġthe', 'Ġcompany', 'Ġgoing', 'Ġto', 'Ġgo', 'Ġgreen', '?']\n",
      " \ttokens_b: ['The', 'Ġcompany', 'Ġis', 'Ġgoing', 'Ġto', 'Ġreduce', 'Ġ8', '%', 'Ġin', 'Ġgas', 'Ġproduction']\n",
      "Features: \n",
      " \tinput_ids: [0, 6209, 5, 138, 164, 7, 213, 2272, 116, 2, 2, 133, 138, 16, 164, 7, 1888, 290, 207, 11, 1123, 931, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "08/26/2020 05:11:17 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 0-0\n",
      "Clear Text: \n",
      " \ttext: Is the company going to go green?\n",
      " \ttext_b: The company is going to reduce 8% in gas production\n",
      "Tokenized: \n",
      " \ttokens: ['Is', 'Ġthe', 'Ġcompany', 'Ġgoing', 'Ġto', 'Ġgo', 'Ġgreen', '?']\n",
      " \ttokens_b: ['The', 'Ġcompany', 'Ġis', 'Ġgoing', 'Ġto', 'Ġreduce', 'Ġ8', '%', 'Ġin', 'Ġgas', 'Ġproduction']\n",
      "Features: \n",
      " \tinput_ids: [0, 6209, 5, 138, 164, 7, 213, 2272, 116, 2, 2, 133, 138, 16, 164, 7, 1888, 290, 207, 11, 1123, 931, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00, 53.75 Batches/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'task': 'text_classification',\n",
       "  'predictions': [{'start': None,\n",
       "    'end': None,\n",
       "    'context': 'Is the company going to go green?|The company is going to reduce 8% in gas production',\n",
       "    'label': '1',\n",
       "    'probability': 0.81757015}]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"The company is going to reduce 8% in gas production\"\n",
    "input_question = \"Is the company going to go green?\"\n",
    "component.run_text(input_text=input_text, input_question=input_question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on an Entire Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_folder()` will make prediction on all the JSON files in the /data/extraction folder. This will take some time, around 35 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "component.run_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are saved in a CSV. For each table, the extracted text, as well as the page number from the source pdf file are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_results = pd.read_csv(\"/model_pipeline/model_pipeline/data/infer/Text.csv\")\n",
    "df_table_results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Table Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to change the `data_type` to `Table`, and make sure that curated table data is present under `FileConfig.curated_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: \n",
      " Table \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_config.data_type = \"Table\"\n",
    "\n",
    "print(f\"Data type: \\n {file_config.data_type} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model_pipeline/model_pipeline/data/curation/esg_TABLE_dataset.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (file_config.curated_table_data)\n",
    "os.path.isfile(file_config.curated_table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as text model, We have already trained a relevance classifier on tables of NQ dataset and the model has been saved under the name `saved_models/NQ/relevence_roberta_table_headers`. \n",
    "ModelConfig.load_dir points to the text checkpoint by feault. It should be changed for table. \n",
    "If set to None, the model will start training from a Roberta Language Model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.load_dir = pathlib.Path(\"/model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta_table_headers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training pipelines for ESG text data and table data are quite similar except the preprocessing. For table, first all the texts inside the tables (column headers, row headers and cells containing text data) should be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "farm_trainer = FARMTrainer(\n",
    "        file_config =file_config,\n",
    "        tokenizer_config=tokenizer_config,\n",
    "        model_config=model_config,\n",
    "        processor_config=processor_config,\n",
    "        training_config=train_config,\n",
    "        mlflow_config=mlflow_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/model_pipeline/model_pipeline/farm_trainer.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.dropna(how=\"any\", inplace=True)\n",
      "/model_pipeline/model_pipeline/farm_trainer.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop_duplicates(inplace=True)\n",
      "08/25/2020 23:47:31 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: True\n",
      "08/25/2020 23:47:31 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'RobertaTokenizer'\n",
      "08/25/2020 23:47:31 - INFO - filelock -   Lock 140156607137616 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
      "08/25/2020 23:47:31 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpa12sufvx\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf1ba16a64d4bb985902f569904d001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:47:32 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "08/25/2020 23:47:32 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "08/25/2020 23:47:32 - INFO - filelock -   Lock 140156607137616 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
      "08/25/2020 23:47:32 - INFO - filelock -   Lock 140156473681104 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "08/25/2020 23:47:32 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpq72q5o4g\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752e97e0f9f5400d995be3c7654c1c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:47:33 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "08/25/2020 23:47:33 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "08/25/2020 23:47:33 - INFO - filelock -   Lock 140156473681104 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "08/25/2020 23:47:33 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "08/25/2020 23:47:33 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "2020/08/25 23:47:33 WARNING mlflow.tracking.context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.data_silo -   \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.data_silo -   Loading train set from: /model_pipeline/model_pipeline/data/train_split_02.csv \n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.data_silo -   Got ya 15 parallel workers to convert 680 dictionaries to pytorch datasets (chunksize = 10)...\n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.data_silo -   /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /|\\  /w\\\n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.data_silo -   / \\  / \\  /'\\  / \\  /'\\  /'\\  /'\\  / \\  / \\  /'\\  /'\\  / \\  /'\\  /'\\  /'\\\n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.data_silo -                               \n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/train_split_02.csv:   0%|          | 0/680 [00:00<?, ? Dicts/s]08/25/2020 23:47:33 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/25/2020 23:47:33 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 5-0\n",
      "Clear Text: \n",
      " \ttext: What is the total amount of direct greenhouse gases emissions referred to as scope 1 emissions?\n",
      " \ttext_b: TOTAL GHG EMISSIONS, Year GHG emissions/revenues*, 2016 124.5, 2017 144.4, 2018 162.4, Scope 1 emissions Scope 2 emissions, (kt CO eq)2 eq) (kt CO 2 eq), 1,203.40 38.90, 1,299.7  37.50, 1,348.83 35.68, Scope 3 emissions, (kt CO 2\n",
      " \ttext_classification_label: 1\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġamount', 'Ġof', 'Ġdirect', 'Ġgreenhouse', 'Ġgases', 'Ġemissions', 'Ġreferred', 'Ġto', 'Ġas', 'Ġscope', 'Ġ1', 'Ġemissions', '?']\n",
      " \ttokens_b: ['T', 'OTAL', 'ĠGH', 'G', 'ĠEM', 'ISS', 'IONS', ',', 'ĠYear', 'ĠGH', 'G', 'Ġemissions', '/', 're', 'ven', 'ues', '*,', 'Ġ2016', 'Ġ124', '.', '5', ',', 'Ġ2017', 'Ġ144', '.', '4', ',', 'Ġ2018', 'Ġ162', '.', '4', ',', 'ĠScope', 'Ġ1', 'Ġemissions', 'ĠScope', 'Ġ2', 'Ġemissions', ',', 'Ġ(', 'kt', 'ĠCO', 'Ġeq', ')', '2', 'Ġeq', ')', 'Ġ(', 'kt', 'ĠCO', 'Ġ2', 'Ġeq', '),', 'Ġ1', ',', '203', '.', '40', 'Ġ38', '.', '90', ',', 'Ġ1', ',', '299', '.', '7', 'Ġ37', '.', '50', ',', 'Ġ1', ',', '348', '.', '83', 'Ġ35', '.', '68', ',', 'ĠScope', 'Ġ3', 'Ġemissions', ',', 'Ġ(', 'kt', 'ĠCO', 'Ġ2']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 1280, 9, 2228, 11832, 20038, 5035, 4997, 7, 25, 7401, 112, 5035, 116, 2, 2, 565, 47778, 22562, 534, 14850, 17588, 11654, 6, 2041, 22562, 534, 5035, 73, 241, 2987, 3663, 36592, 336, 19446, 4, 245, 6, 193, 19641, 4, 306, 6, 199, 25728, 4, 306, 6, 30108, 112, 5035, 30108, 132, 5035, 6, 36, 7282, 6247, 48234, 43, 176, 48234, 43, 36, 7282, 6247, 132, 48234, 238, 112, 6, 25534, 4, 1749, 2843, 4, 3248, 6, 112, 6, 23942, 4, 406, 2908, 4, 1096, 6, 112, 6, 27765, 4, 6361, 1718, 4, 4671, 6, 30108, 155, 5035, 6, 36, 7282, 6247, 132, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:47:33 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 5-0\n",
      "Clear Text: \n",
      " \ttext: What is the total amount of direct greenhouse gases emissions referred to as scope 1 emissions?\n",
      " \ttext_b: TOTAL GHG EMISSIONS, Year GHG emissions/revenues*, 2016 124.5, 2017 144.4, 2018 162.4, Scope 1 emissions Scope 2 emissions, (kt CO eq)2 eq) (kt CO 2 eq), 1,203.40 38.90, 1,299.7  37.50, 1,348.83 35.68, Scope 3 emissions, (kt CO 2\n",
      " \ttext_classification_label: 1\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġamount', 'Ġof', 'Ġdirect', 'Ġgreenhouse', 'Ġgases', 'Ġemissions', 'Ġreferred', 'Ġto', 'Ġas', 'Ġscope', 'Ġ1', 'Ġemissions', '?']\n",
      " \ttokens_b: ['T', 'OTAL', 'ĠGH', 'G', 'ĠEM', 'ISS', 'IONS', ',', 'ĠYear', 'ĠGH', 'G', 'Ġemissions', '/', 're', 'ven', 'ues', '*,', 'Ġ2016', 'Ġ124', '.', '5', ',', 'Ġ2017', 'Ġ144', '.', '4', ',', 'Ġ2018', 'Ġ162', '.', '4', ',', 'ĠScope', 'Ġ1', 'Ġemissions', 'ĠScope', 'Ġ2', 'Ġemissions', ',', 'Ġ(', 'kt', 'ĠCO', 'Ġeq', ')', '2', 'Ġeq', ')', 'Ġ(', 'kt', 'ĠCO', 'Ġ2', 'Ġeq', '),', 'Ġ1', ',', '203', '.', '40', 'Ġ38', '.', '90', ',', 'Ġ1', ',', '299', '.', '7', 'Ġ37', '.', '50', ',', 'Ġ1', ',', '348', '.', '83', 'Ġ35', '.', '68', ',', 'ĠScope', 'Ġ3', 'Ġemissions', ',', 'Ġ(', 'kt', 'ĠCO', 'Ġ2']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 1280, 9, 2228, 11832, 20038, 5035, 4997, 7, 25, 7401, 112, 5035, 116, 2, 2, 565, 47778, 22562, 534, 14850, 17588, 11654, 6, 2041, 22562, 534, 5035, 73, 241, 2987, 3663, 36592, 336, 19446, 4, 245, 6, 193, 19641, 4, 306, 6, 199, 25728, 4, 306, 6, 30108, 112, 5035, 30108, 132, 5035, 6, 36, 7282, 6247, 48234, 43, 176, 48234, 43, 36, 7282, 6247, 132, 48234, 238, 112, 6, 25534, 4, 1749, 2843, 4, 3248, 6, 112, 6, 23942, 4, 406, 2908, 4, 1096, 6, 112, 6, 27765, 4, 6361, 1718, 4, 4671, 6, 30108, 155, 5035, 6, 36, 7282, 6247, 132, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [1]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/train_split_02.csv: 100%|██████████| 680/680 [00:05<00:00, 121.40 Dicts/s]\n",
      "08/25/2020 23:47:39 - INFO - farm.data_handler.data_silo -   Loading dev set from: /model_pipeline/model_pipeline/data/val_split_02.csv\n",
      "08/25/2020 23:47:39 - INFO - farm.data_handler.data_silo -   Got ya 15 parallel workers to convert 171 dictionaries to pytorch datasets (chunksize = 4)...\n",
      "08/25/2020 23:47:39 - INFO - farm.data_handler.data_silo -    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n",
      "08/25/2020 23:47:39 - INFO - farm.data_handler.data_silo -   /|\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /w\\  /|\\  /w\\  /w\\  /w\\  /w\\  /w\\  /w\\\n",
      "08/25/2020 23:47:39 - INFO - farm.data_handler.data_silo -   /'\\  / \\  /'\\  / \\  / \\  /'\\  /'\\  /'\\  /'\\  / \\  / \\  / \\  / \\  /'\\  /'\\\n",
      "08/25/2020 23:47:39 - INFO - farm.data_handler.data_silo -                               \n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/val_split_02.csv:   0%|          | 0/171 [00:00<?, ? Dicts/s]08/25/2020 23:47:39 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/25/2020 23:47:39 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1-0\n",
      "Clear Text: \n",
      " \ttext: What is the total amount of energy indirect greenhouse gases emissions referred to as scope 2 emissions?\n",
      " \ttext_b: Estimated total energy (MJ) delivered by Shell [A], Estimated greenhouse gas emissions covered by the Net Carbon, Footprint calculation (million tonnes CO2e) [B], 2.105E+13, 2.200E+13, 2.144E+13, 2.093E+13\n",
      " \ttext_classification_label: 0\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġamount', 'Ġof', 'Ġenergy', 'Ġindirect', 'Ġgreenhouse', 'Ġgases', 'Ġemissions', 'Ġreferred', 'Ġto', 'Ġas', 'Ġscope', 'Ġ2', 'Ġemissions', '?']\n",
      " \ttokens_b: ['Est', 'imated', 'Ġtotal', 'Ġenergy', 'Ġ(', 'MJ', ')', 'Ġdelivered', 'Ġby', 'ĠShell', 'Ġ[', 'A', '],', 'ĠEstimated', 'Ġgreenhouse', 'Ġgas', 'Ġemissions', 'Ġcovered', 'Ġby', 'Ġthe', 'ĠNet', 'ĠCarbon', ',', 'ĠFoot', 'print', 'Ġcalculation', 'Ġ(', 'million', 'Ġtonnes', 'ĠCO', '2', 'e', ')', 'Ġ[', 'B', '],', 'Ġ2', '.', '105', 'E', '+', '13', ',', 'Ġ2', '.', '200', 'E', '+', '13', ',', 'Ġ2', '.', '144', 'E', '+', '13', ',', 'Ġ2', '.', '09', '3', 'E', '+', '13']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 1280, 9, 1007, 18677, 11832, 20038, 5035, 4997, 7, 25, 7401, 132, 5035, 116, 2, 2, 27602, 24985, 746, 1007, 36, 41062, 43, 2781, 30, 10700, 646, 250, 7479, 33991, 11832, 1123, 5035, 2913, 30, 5, 5008, 17962, 6, 13324, 17265, 21586, 36, 4416, 5657, 6247, 176, 242, 43, 646, 387, 7479, 132, 4, 15274, 717, 2744, 1558, 6, 132, 4, 2619, 717, 2744, 1558, 6, 132, 4, 25208, 717, 2744, 1558, 6, 132, 4, 3546, 246, 717, 2744, 1558, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:47:39 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 1-0\n",
      "Clear Text: \n",
      " \ttext: What is the total amount of energy indirect greenhouse gases emissions referred to as scope 2 emissions?\n",
      " \ttext_b: Estimated total energy (MJ) delivered by Shell [A], Estimated greenhouse gas emissions covered by the Net Carbon, Footprint calculation (million tonnes CO2e) [B], 2.105E+13, 2.200E+13, 2.144E+13, 2.093E+13\n",
      " \ttext_classification_label: 0\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtotal', 'Ġamount', 'Ġof', 'Ġenergy', 'Ġindirect', 'Ġgreenhouse', 'Ġgases', 'Ġemissions', 'Ġreferred', 'Ġto', 'Ġas', 'Ġscope', 'Ġ2', 'Ġemissions', '?']\n",
      " \ttokens_b: ['Est', 'imated', 'Ġtotal', 'Ġenergy', 'Ġ(', 'MJ', ')', 'Ġdelivered', 'Ġby', 'ĠShell', 'Ġ[', 'A', '],', 'ĠEstimated', 'Ġgreenhouse', 'Ġgas', 'Ġemissions', 'Ġcovered', 'Ġby', 'Ġthe', 'ĠNet', 'ĠCarbon', ',', 'ĠFoot', 'print', 'Ġcalculation', 'Ġ(', 'million', 'Ġtonnes', 'ĠCO', '2', 'e', ')', 'Ġ[', 'B', '],', 'Ġ2', '.', '105', 'E', '+', '13', ',', 'Ġ2', '.', '200', 'E', '+', '13', ',', 'Ġ2', '.', '144', 'E', '+', '13', ',', 'Ġ2', '.', '09', '3', 'E', '+', '13']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 746, 1280, 9, 1007, 18677, 11832, 20038, 5035, 4997, 7, 25, 7401, 132, 5035, 116, 2, 2, 27602, 24985, 746, 1007, 36, 41062, 43, 2781, 30, 10700, 646, 250, 7479, 33991, 11832, 1123, 5035, 2913, 30, 5, 5008, 17962, 6, 13324, 17265, 21586, 36, 4416, 5657, 6247, 176, 242, 43, 646, 387, 7479, 132, 4, 15274, 717, 2744, 1558, 6, 132, 4, 2619, 717, 2744, 1558, 6, 132, 4, 25208, 717, 2744, 1558, 6, 132, 4, 3546, 246, 717, 2744, 1558, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \ttext_classification_label_ids: [0]\n",
      "_____________________________________________________\n",
      "Preprocessing Dataset /model_pipeline/model_pipeline/data/val_split_02.csv: 100%|██████████| 171/171 [00:03<00:00, 51.20 Dicts/s]\n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   No test set is being loaded\n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   Examples in train: 680\n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   Examples in dev  : 171\n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   Examples in test : 0\n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   \n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     512\n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 201.33235294117648\n",
      "08/25/2020 23:47:42 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.11176470588235295\n",
      "08/25/2020 23:47:42 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "08/25/2020 23:47:43 - INFO - filelock -   Lock 140156367390736 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690.lock\n",
      "08/25/2020 23:47:43 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8hoflw_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3807472fae8b4714af57de33fd14455a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:47:43 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "08/25/2020 23:47:43 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "08/25/2020 23:47:43 - INFO - filelock -   Lock 140156367390736 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690.lock\n",
      "08/25/2020 23:47:43 - INFO - filelock -   Lock 140156367273936 acquired on /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
      "08/25/2020 23:47:43 - INFO - transformers.file_utils -   https://cdn.huggingface.co/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpsrnzz01x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a985551974d9bbb6700f2c48b61df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:48:00 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "08/25/2020 23:48:00 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "08/25/2020 23:48:00 - INFO - filelock -   Lock 140156367273936 released on /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
      "08/25/2020 23:48:00 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "08/25/2020 23:48:05 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing RobertaModel.\n",
      "\n",
      "08/25/2020 23:48:05 - INFO - transformers.modeling_utils -   All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "08/25/2020 23:48:05 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "\t We guess it's an *ENGLISH* model ... \n",
      "\t If not: Init the language model by supplying the 'language' param.\n",
      "08/25/2020 23:48:08 - INFO - transformers.modeling_utils -   loading weights file /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta_table_headers/language_model.bin from cache at /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta_table_headers/language_model.bin\n",
      "08/25/2020 23:48:13 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing RobertaModel.\n",
      "\n",
      "08/25/2020 23:48:13 - INFO - transformers.modeling_utils -   All the weights of RobertaModel were initialized from the model checkpoint at /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta_table_headers/language_model.bin.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "08/25/2020 23:48:13 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "08/25/2020 23:48:13 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "08/25/2020 23:48:13 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "08/25/2020 23:48:13 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [1.0, 1.0]\n",
      "08/25/2020 23:48:13 - INFO - farm.modeling.prediction_head -   Loading prediction head from /model_pipeline/model_pipeline/saved_models/NQ/relevance_roberta_table_headers/prediction_head_0.bin\n",
      "08/25/2020 23:48:13 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1.2168533249479066e-05}'\n",
      "08/25/2020 23:48:14 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
      "08/25/2020 23:48:14 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 85.0, 'num_training_steps': 850}'\n",
      "08/25/2020 23:48:14 - INFO - farm.train -   \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 0/4 (Cur. train loss: 0.0198):  18%|█▊        | 30/170 [00:08<00:37,  3.78it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.00it/s]\n",
      "08/25/2020 23:48:25 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 30 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:48:25 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:48:25 - INFO - farm.eval -   loss: 0.6772606885224058\n",
      "08/25/2020 23:48:25 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:48:25 - INFO - farm.eval -   acc: 0.8538011695906432\n",
      "08/25/2020 23:48:25 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8828    0.9187    0.9004       123\n",
      "           1     0.7674    0.6875    0.7253        48\n",
      "\n",
      "    accuracy                         0.8538       171\n",
      "   macro avg     0.8251    0.8031    0.8128       171\n",
      "weighted avg     0.8504    0.8538    0.8512       171\n",
      "\n",
      "Train epoch 0/4 (Cur. train loss: 0.0079):  35%|███▌      | 60/170 [00:19<00:28,  3.92it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.01it/s]\n",
      "08/25/2020 23:48:36 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 60 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:48:36 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:48:36 - INFO - farm.eval -   loss: 0.5987991582580477\n",
      "08/25/2020 23:48:36 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:48:36 - INFO - farm.eval -   acc: 0.8654970760233918\n",
      "08/25/2020 23:48:36 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9098    0.9024    0.9061       123\n",
      "           1     0.7551    0.7708    0.7629        48\n",
      "\n",
      "    accuracy                         0.8655       171\n",
      "   macro avg     0.8325    0.8366    0.8345       171\n",
      "weighted avg     0.8664    0.8655    0.8659       171\n",
      "\n",
      "Train epoch 0/4 (Cur. train loss: 1.3849):  53%|█████▎    | 90/170 [00:30<00:20,  3.90it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:48:47 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 90 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:48:47 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:48:47 - INFO - farm.eval -   loss: 0.4233477664621253\n",
      "08/25/2020 23:48:47 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:48:47 - INFO - farm.eval -   acc: 0.8888888888888888\n",
      "08/25/2020 23:48:47 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9262    0.9187    0.9224       123\n",
      "           1     0.7959    0.8125    0.8041        48\n",
      "\n",
      "    accuracy                         0.8889       171\n",
      "   macro avg     0.8611    0.8656    0.8633       171\n",
      "weighted avg     0.8897    0.8889    0.8892       171\n",
      "\n",
      "Train epoch 0/4 (Cur. train loss: 0.0836):  71%|███████   | 120/170 [00:41<00:12,  3.88it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.01it/s]\n",
      "08/25/2020 23:48:58 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 120 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:48:58 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:48:58 - INFO - farm.eval -   loss: 0.41673625700655037\n",
      "08/25/2020 23:48:58 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:48:58 - INFO - farm.eval -   acc: 0.847953216374269\n",
      "08/25/2020 23:48:58 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9709    0.8130    0.8850       123\n",
      "           1     0.6618    0.9375    0.7759        48\n",
      "\n",
      "    accuracy                         0.8480       171\n",
      "   macro avg     0.8163    0.8753    0.8304       171\n",
      "weighted avg     0.8841    0.8480    0.8543       171\n",
      "\n",
      "Train epoch 0/4 (Cur. train loss: 0.0330):  88%|████████▊ | 150/170 [00:51<00:05,  3.88it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:49:09 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 150 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:49:09 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:49:09 - INFO - farm.eval -   loss: 0.33647545039305216\n",
      "08/25/2020 23:49:09 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:49:09 - INFO - farm.eval -   acc: 0.8888888888888888\n",
      "08/25/2020 23:49:09 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9407    0.9024    0.9212       123\n",
      "           1     0.7736    0.8542    0.8119        48\n",
      "\n",
      "    accuracy                         0.8889       171\n",
      "   macro avg     0.8571    0.8783    0.8665       171\n",
      "weighted avg     0.8938    0.8889    0.8905       171\n",
      "\n",
      "Train epoch 0/4 (Cur. train loss: 0.0052): 100%|██████████| 170/170 [01:00<00:00,  2.82it/s]\n",
      "Train epoch 1/4 (Cur. train loss: 0.0205):   6%|▌         | 10/170 [00:02<00:41,  3.86it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.03it/s]\n",
      "08/25/2020 23:49:20 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 180 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:49:20 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:49:20 - INFO - farm.eval -   loss: 0.4943087393777412\n",
      "08/25/2020 23:49:20 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:49:20 - INFO - farm.eval -   acc: 0.8771929824561403\n",
      "08/25/2020 23:49:20 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9322    0.8943    0.9129       123\n",
      "           1     0.7547    0.8333    0.7921        48\n",
      "\n",
      "    accuracy                         0.8772       171\n",
      "   macro avg     0.8435    0.8638    0.8525       171\n",
      "weighted avg     0.8824    0.8772    0.8790       171\n",
      "\n",
      "Train epoch 1/4 (Cur. train loss: 0.0045):  24%|██▎       | 40/170 [00:13<00:33,  3.94it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.01it/s]\n",
      "08/25/2020 23:49:31 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 210 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:49:31 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:49:31 - INFO - farm.eval -   loss: 0.5794098286600838\n",
      "08/25/2020 23:49:31 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:49:31 - INFO - farm.eval -   acc: 0.8771929824561403\n",
      "08/25/2020 23:49:31 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9636    0.8618    0.9099       123\n",
      "           1     0.7213    0.9167    0.8073        48\n",
      "\n",
      "    accuracy                         0.8772       171\n",
      "   macro avg     0.8425    0.8892    0.8586       171\n",
      "weighted avg     0.8956    0.8772    0.8811       171\n",
      "\n",
      "Train epoch 1/4 (Cur. train loss: 0.0007):  41%|████      | 70/170 [00:24<00:25,  3.94it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:49:42 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 240 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:49:42 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:49:42 - INFO - farm.eval -   loss: 0.5750115761282848\n",
      "08/25/2020 23:49:42 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:49:42 - INFO - farm.eval -   acc: 0.8888888888888888\n",
      "08/25/2020 23:49:42 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9194    0.9268    0.9231       123\n",
      "           1     0.8085    0.7917    0.8000        48\n",
      "\n",
      "    accuracy                         0.8889       171\n",
      "   macro avg     0.8639    0.8592    0.8615       171\n",
      "weighted avg     0.8882    0.8889    0.8885       171\n",
      "\n",
      "Train epoch 1/4 (Cur. train loss: 0.0025):  59%|█████▉    | 100/170 [00:35<00:17,  3.94it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:49:53 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 270 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:49:53 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:49:53 - INFO - farm.eval -   loss: 0.509664853762465\n",
      "08/25/2020 23:49:53 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:49:53 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:49:53 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9573    0.9106    0.9333       123\n",
      "           1     0.7963    0.8958    0.8431        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8768    0.9032    0.8882       171\n",
      "weighted avg     0.9121    0.9064    0.9080       171\n",
      "\n",
      "Train epoch 1/4 (Cur. train loss: 2.5883):  76%|███████▋  | 130/170 [00:46<00:10,  3.89it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:50:03 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 300 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:50:03 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:50:03 - INFO - farm.eval -   loss: 0.5068952876921983\n",
      "08/25/2020 23:50:03 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:50:04 - INFO - farm.eval -   acc: 0.8888888888888888\n",
      "08/25/2020 23:50:04 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9194    0.9268    0.9231       123\n",
      "           1     0.8085    0.7917    0.8000        48\n",
      "\n",
      "    accuracy                         0.8889       171\n",
      "   macro avg     0.8639    0.8592    0.8615       171\n",
      "weighted avg     0.8882    0.8889    0.8885       171\n",
      "\n",
      "Train epoch 1/4 (Cur. train loss: 1.1553):  94%|█████████▍| 160/170 [00:56<00:02,  3.93it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.04it/s]\n",
      "08/25/2020 23:50:14 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 330 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:50:14 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:50:14 - INFO - farm.eval -   loss: 0.5405659410688612\n",
      "08/25/2020 23:50:14 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:50:14 - INFO - farm.eval -   acc: 0.8830409356725146\n",
      "08/25/2020 23:50:14 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9558    0.8780    0.9153       123\n",
      "           1     0.7414    0.8958    0.8113        48\n",
      "\n",
      "    accuracy                         0.8830       171\n",
      "   macro avg     0.8486    0.8869    0.8633       171\n",
      "weighted avg     0.8956    0.8830    0.8861       171\n",
      "\n",
      "Train epoch 1/4 (Cur. train loss: 1.0775): 100%|██████████| 170/170 [01:02<00:00,  2.71it/s]\n",
      "Train epoch 2/4 (Cur. train loss: 0.0022):  12%|█▏        | 20/170 [00:05<00:38,  3.92it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:50:25 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 360 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:50:25 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:50:25 - INFO - farm.eval -   loss: 0.5126257605022855\n",
      "08/25/2020 23:50:25 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:50:25 - INFO - farm.eval -   acc: 0.8888888888888888\n",
      "08/25/2020 23:50:25 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9333    0.9106    0.9218       123\n",
      "           1     0.7843    0.8333    0.8081        48\n",
      "\n",
      "    accuracy                         0.8889       171\n",
      "   macro avg     0.8588    0.8720    0.8649       171\n",
      "weighted avg     0.8915    0.8889    0.8899       171\n",
      "\n",
      "Train epoch 2/4 (Cur. train loss: 0.0049):  29%|██▉       | 50/170 [00:16<00:30,  3.93it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:50:36 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 390 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:50:36 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:50:36 - INFO - farm.eval -   loss: 0.3926999060034055\n",
      "08/25/2020 23:50:36 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:50:36 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:50:36 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9573    0.9106    0.9333       123\n",
      "           1     0.7963    0.8958    0.8431        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8768    0.9032    0.8882       171\n",
      "weighted avg     0.9121    0.9064    0.9080       171\n",
      "\n",
      "Train epoch 2/4 (Cur. train loss: 0.0062):  47%|████▋     | 80/170 [00:26<00:22,  3.94it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.01it/s]\n",
      "08/25/2020 23:50:47 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 420 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:50:47 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:50:47 - INFO - farm.eval -   loss: 0.6389309913791411\n",
      "08/25/2020 23:50:47 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:50:47 - INFO - farm.eval -   acc: 0.8830409356725146\n",
      "08/25/2020 23:50:47 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9256    0.9106    0.9180       123\n",
      "           1     0.7800    0.8125    0.7959        48\n",
      "\n",
      "    accuracy                         0.8830       171\n",
      "   macro avg     0.8528    0.8615    0.8570       171\n",
      "weighted avg     0.8847    0.8830    0.8838       171\n",
      "\n",
      "Train epoch 2/4 (Cur. train loss: 1.8460):  65%|██████▍   | 110/170 [00:37<00:15,  3.92it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:50:58 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 450 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:50:58 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:50:58 - INFO - farm.eval -   loss: 0.623805702778331\n",
      "08/25/2020 23:50:58 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:50:58 - INFO - farm.eval -   acc: 0.8888888888888888\n",
      "08/25/2020 23:50:58 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9262    0.9187    0.9224       123\n",
      "           1     0.7959    0.8125    0.8041        48\n",
      "\n",
      "    accuracy                         0.8889       171\n",
      "   macro avg     0.8611    0.8656    0.8633       171\n",
      "weighted avg     0.8897    0.8889    0.8892       171\n",
      "\n",
      "Train epoch 2/4 (Cur. train loss: 0.0351):  82%|████████▏ | 140/170 [00:48<00:07,  3.90it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.01it/s]\n",
      "08/25/2020 23:51:09 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 480 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:51:09 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:51:09 - INFO - farm.eval -   loss: 0.45005227808366743\n",
      "08/25/2020 23:51:09 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:51:09 - INFO - farm.eval -   acc: 0.9005847953216374\n",
      "08/25/2020 23:51:09 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9417    0.9187    0.9300       123\n",
      "           1     0.8039    0.8542    0.8283        48\n",
      "\n",
      "    accuracy                         0.9006       171\n",
      "   macro avg     0.8728    0.8864    0.8792       171\n",
      "weighted avg     0.9030    0.9006    0.9015       171\n",
      "\n",
      "Train epoch 2/4 (Cur. train loss: 0.0031): 100%|██████████| 170/170 [00:59<00:00,  2.84it/s]\n",
      "Train epoch 3/4 (Cur. train loss: 0.0010):   0%|          | 0/170 [00:00<?, ?it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:51:20 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 510 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:51:20 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:51:20 - INFO - farm.eval -   loss: 0.45537774325811375\n",
      "08/25/2020 23:51:20 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:51:20 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:51:20 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9573    0.9106    0.9333       123\n",
      "           1     0.7963    0.8958    0.8431        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8768    0.9032    0.8882       171\n",
      "weighted avg     0.9121    0.9064    0.9080       171\n",
      "\n",
      "Train epoch 3/4 (Cur. train loss: 0.0029):  18%|█▊        | 30/170 [00:10<00:35,  3.95it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.03it/s]\n",
      "08/25/2020 23:51:31 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 540 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:51:31 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:51:31 - INFO - farm.eval -   loss: 0.4954990253113864\n",
      "08/25/2020 23:51:31 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:51:31 - INFO - farm.eval -   acc: 0.9005847953216374\n",
      "08/25/2020 23:51:31 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9417    0.9187    0.9300       123\n",
      "           1     0.8039    0.8542    0.8283        48\n",
      "\n",
      "    accuracy                         0.9006       171\n",
      "   macro avg     0.8728    0.8864    0.8792       171\n",
      "weighted avg     0.9030    0.9006    0.9015       171\n",
      "\n",
      "Train epoch 3/4 (Cur. train loss: 0.0073):  35%|███▌      | 60/170 [00:21<00:27,  3.93it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:51:42 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 570 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:51:42 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:51:42 - INFO - farm.eval -   loss: 0.45246846006627667\n",
      "08/25/2020 23:51:42 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:51:42 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:51:42 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9573    0.9106    0.9333       123\n",
      "           1     0.7963    0.8958    0.8431        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8768    0.9032    0.8882       171\n",
      "weighted avg     0.9121    0.9064    0.9080       171\n",
      "\n",
      "Train epoch 3/4 (Cur. train loss: 0.0008):  53%|█████▎    | 90/170 [00:32<00:20,  3.97it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:51:52 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 600 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:51:52 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:51:52 - INFO - farm.eval -   loss: 0.5089994522563198\n",
      "08/25/2020 23:51:52 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:51:53 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:51:53 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9652    0.9024    0.9328       123\n",
      "           1     0.7857    0.9167    0.8462        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8755    0.9096    0.8895       171\n",
      "weighted avg     0.9148    0.9064    0.9085       171\n",
      "\n",
      "Train epoch 3/4 (Cur. train loss: 0.0046):  71%|███████   | 120/170 [00:43<00:12,  3.93it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.03it/s]\n",
      "08/25/2020 23:52:03 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 630 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:52:03 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:52:03 - INFO - farm.eval -   loss: 0.4831775970626296\n",
      "08/25/2020 23:52:03 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:52:03 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:52:03 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9496    0.9187    0.9339       123\n",
      "           1     0.8077    0.8750    0.8400        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8786    0.8968    0.8869       171\n",
      "weighted avg     0.9098    0.9064    0.9075       171\n",
      "\n",
      "Train epoch 3/4 (Cur. train loss: 0.0005):  88%|████████▊ | 150/170 [00:54<00:05,  3.94it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.00it/s]\n",
      "08/25/2020 23:52:14 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 660 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:52:14 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:52:14 - INFO - farm.eval -   loss: 0.43524420453093904\n",
      "08/25/2020 23:52:14 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:52:14 - INFO - farm.eval -   acc: 0.9181286549707602\n",
      "08/25/2020 23:52:14 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9431    0.9431    0.9431       123\n",
      "           1     0.8542    0.8542    0.8542        48\n",
      "\n",
      "    accuracy                         0.9181       171\n",
      "   macro avg     0.8986    0.8986    0.8986       171\n",
      "weighted avg     0.9181    0.9181    0.9181       171\n",
      "\n",
      "Train epoch 3/4 (Cur. train loss: 0.0354): 100%|██████████| 170/170 [01:02<00:00,  2.71it/s]\n",
      "Train epoch 4/4 (Cur. train loss: 0.0004):   6%|▌         | 10/170 [00:02<00:40,  3.92it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:52:25 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 690 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:52:25 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:52:25 - INFO - farm.eval -   loss: 0.4542162711160225\n",
      "08/25/2020 23:52:25 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:52:25 - INFO - farm.eval -   acc: 0.9122807017543859\n",
      "08/25/2020 23:52:25 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9426    0.9350    0.9388       123\n",
      "           1     0.8367    0.8542    0.8454        48\n",
      "\n",
      "    accuracy                         0.9123       171\n",
      "   macro avg     0.8897    0.8946    0.8921       171\n",
      "weighted avg     0.9129    0.9123    0.9126       171\n",
      "\n",
      "Train epoch 4/4 (Cur. train loss: 0.6336):  24%|██▎       | 40/170 [00:13<00:33,  3.92it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:52:36 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 720 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:52:36 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:52:36 - INFO - farm.eval -   loss: 0.47753859333127563\n",
      "08/25/2020 23:52:36 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:52:36 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:52:36 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9421    0.9268    0.9344       123\n",
      "           1     0.8200    0.8542    0.8367        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8811    0.8905    0.8856       171\n",
      "weighted avg     0.9079    0.9064    0.9070       171\n",
      "\n",
      "Train epoch 4/4 (Cur. train loss: 0.0008):  41%|████      | 70/170 [00:24<00:25,  3.95it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:52:47 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 750 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:52:47 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:52:47 - INFO - farm.eval -   loss: 0.4788741547810404\n",
      "08/25/2020 23:52:47 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:52:47 - INFO - farm.eval -   acc: 0.9122807017543859\n",
      "08/25/2020 23:52:47 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9500    0.9268    0.9383       123\n",
      "           1     0.8235    0.8750    0.8485        48\n",
      "\n",
      "    accuracy                         0.9123       171\n",
      "   macro avg     0.8868    0.9009    0.8934       171\n",
      "weighted avg     0.9145    0.9123    0.9131       171\n",
      "\n",
      "Train epoch 4/4 (Cur. train loss: 0.0016):  59%|█████▉    | 100/170 [00:35<00:17,  3.96it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:52:58 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 780 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:52:58 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:52:58 - INFO - farm.eval -   loss: 0.4784531136702376\n",
      "08/25/2020 23:52:58 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:52:58 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:52:58 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9421    0.9268    0.9344       123\n",
      "           1     0.8200    0.8542    0.8367        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8811    0.8905    0.8856       171\n",
      "weighted avg     0.9079    0.9064    0.9070       171\n",
      "\n",
      "Train epoch 4/4 (Cur. train loss: 0.9867):  76%|███████▋  | 130/170 [00:46<00:10,  3.92it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:53:09 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 810 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:53:09 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:53:09 - INFO - farm.eval -   loss: 0.474899018717091\n",
      "08/25/2020 23:53:09 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:53:09 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:53:09 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9421    0.9268    0.9344       123\n",
      "           1     0.8200    0.8542    0.8367        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8811    0.8905    0.8856       171\n",
      "weighted avg     0.9079    0.9064    0.9070       171\n",
      "\n",
      "Train epoch 4/4 (Cur. train loss: 0.5153):  94%|█████████▍| 160/170 [00:56<00:02,  3.94it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.03it/s]\n",
      "08/25/2020 23:53:19 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 840 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:53:19 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:53:19 - INFO - farm.eval -   loss: 0.4744530592048377\n",
      "08/25/2020 23:53:19 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:53:20 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:53:20 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9421    0.9268    0.9344       123\n",
      "           1     0.8200    0.8542    0.8367        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8811    0.8905    0.8856       171\n",
      "weighted avg     0.9079    0.9064    0.9070       171\n",
      "\n",
      "Train epoch 4/4 (Cur. train loss: 0.0030): 100%|██████████| 170/170 [01:02<00:00,  2.71it/s]\n",
      "Evaluating: 100%|██████████| 43/43 [00:03<00:00, 14.02it/s]\n",
      "08/25/2020 23:53:25 - INFO - farm.eval -   \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 43 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "08/25/2020 23:53:25 - INFO - farm.eval -   \n",
      " _________ text_classification _________\n",
      "08/25/2020 23:53:25 - INFO - farm.eval -   loss: 0.47376243517412775\n",
      "08/25/2020 23:53:25 - INFO - farm.eval -   task_name: text_classification\n",
      "08/25/2020 23:53:25 - INFO - farm.eval -   acc: 0.9064327485380117\n",
      "08/25/2020 23:53:25 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9421    0.9268    0.9344       123\n",
      "           1     0.8200    0.8542    0.8367        48\n",
      "\n",
      "    accuracy                         0.9064       171\n",
      "   macro avg     0.8811    0.8905    0.8856       171\n",
      "weighted avg     0.9079    0.9064    0.9070       171\n",
      "\n",
      "08/25/2020 23:53:26 - INFO - model_pipeline.farm_trainer -   Trained model saved to /model_pipeline/model_pipeline/saved_models/test_farm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/25/2020 23:53:26 - INFO - model_pipeline.farm_trainer -   Processor vocabulary saved to /model_pipeline/model_pipeline/saved_models/test_farm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9064327485380117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farm_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be save in the FileConfig.saved_models_dir direcory and we can use it for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Table model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Inference, \n",
    "1. The data needs to already be extarcted by the extraction component.\n",
    "2. The model need to be trained and its checkpoints needs to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_pipeline.relevance_infer import TableRelevanceInfer\n",
    "from model_pipeline.config_farm_train import InferConfig\n",
    "import pathlib, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference component expects to find the trained model in \"Table\" key of `infer_config.load_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_config = InferConfig()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Table': 'saved_models/test_farm/Table',\n",
       " 'Text': 'saved_models/test_farm/Text'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_config.load_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can load the model pretrained by 1QBit using the following cell. Skip if you want to use yout own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneqbit_checkpoint_dir = pathlib.Path(\"/model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG\")\n",
    "infer_config.load_dir = {\"Text\": oneqbit_checkpoint_dir / \"esg_text_checkpoint\",\n",
    "                \"Table\": oneqbit_checkpoint_dir / \"esg_table_checkpoint\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/19/2020 21:15:28 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n",
      "08/19/2020 21:15:28 - INFO - transformers.modeling_utils -   loading weights file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/language_model.bin from cache at /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/language_model.bin\n",
      "08/19/2020 21:15:33 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing RobertaModel.\n",
      "\n",
      "08/19/2020 21:15:33 - INFO - transformers.modeling_utils -   All the weights of RobertaModel were initialized from the model checkpoint at /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/language_model.bin.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "08/19/2020 21:15:33 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
      "08/19/2020 21:15:33 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
      "08/19/2020 21:15:33 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 2]\n",
      "08/19/2020 21:15:33 - INFO - farm.modeling.prediction_head -   Using class weights for task 'text_classification': [1.0, 1.0]\n",
      "08/19/2020 21:15:33 - INFO - farm.modeling.prediction_head -   Loading prediction head from /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/prediction_head_0.bin\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   Model name '/model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   Didn't find file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/added_tokens.json. We won't load it.\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   Didn't find file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/tokenizer.json. We won't load it.\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/vocab.json\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/merges.txt\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/special_tokens_map.json\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   loading file /model_pipeline/model_pipeline/saved_models/1QBit_Pretrained_ESG/esg_table_checkpoint/tokenizer_config.json\n",
      "08/19/2020 21:15:33 - INFO - transformers.tokenization_utils_base -   loading file None\n",
      "08/19/2020 21:15:33 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "08/19/2020 21:15:33 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n"
     ]
    }
   ],
   "source": [
    "component = TableRelevanceInfer(infer_config) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on an Entire Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_folder` in `TableRelevanceInfer` is the method responsile method for making prediction on all the csv files located in a folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page102_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page104_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page105_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page108_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page109_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page109_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page110_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page110_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page112_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page112_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page113_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page113_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page114_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page114_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page115_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page116_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page116_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page123_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page130_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page130_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page131_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page131_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page131_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page131_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page133_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page134_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page136_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page140_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page141_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page142_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page145_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page145_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page154_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page161_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page162_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page165_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page169_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page16_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page170_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page171_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page172_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page173_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page184_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page184_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page185_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page185_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page186_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page186_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page187_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page187_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page188_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page188_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page189_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page189_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page190_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page190_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page191_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page191_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page192_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page192_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page192_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page192_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page192_5.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page192_6.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page194_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page194_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page195_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page195_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page195_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page195_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page196_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page196_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page196_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page196_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page197_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page197_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page197_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page198_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page198_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page199_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page199_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page200_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page200_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page201_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page201_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page202_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page202_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page203_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page203_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page203_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page203_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page204_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page204_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page204_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page204_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page205_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page206_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page208_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page208_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page209_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page209_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page20_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page210_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page210_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page211_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page211_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page211_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page211_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page212_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page213_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page213_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page213_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page213_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page214_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page214_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page215_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page215_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page215_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page215_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page216_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page216_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page216_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page217_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page217_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page218_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page218_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page219_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page21_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page220_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page221_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page222_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page224_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page224_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page225_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page225_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page225_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page226_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page226_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page226_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page226_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page227_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page228_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page229_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page22_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page230_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page230_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page230_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page230_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page231_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page232_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page233_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page233_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page233_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page234_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page235_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page236_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page237_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page238_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page238_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page238_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page238_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page238_5.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page239_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page23_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page240_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page240_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page241_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page241_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page242_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page242_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page242_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page242_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page243_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page246_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page247_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page248_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page249_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page250_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page251_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page252_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page253_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page254_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page255_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page256_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page257_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page258_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page259_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page260_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page261_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page262_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page265_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page265_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page266_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page267_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page267_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page268_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page269_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page270_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page270_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page271_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page272_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page273_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page274_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page274_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page275_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page275_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page276_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page277_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page277_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page277_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page278_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page278_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page278_3.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page278_4.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page279_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page279_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page281_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page282_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page283_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page284_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page285_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page286_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page287_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page287_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page288_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page289_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page290_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page291_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page292_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page31_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page31_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page32_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page32_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page33_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page38_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page38_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page39_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page39_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page3_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page41_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page42_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page43_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page49_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page50_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page55_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page56_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page57_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page57_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page58_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page5_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page61_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page61_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page62_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page64_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page65_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page72_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page7_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page82_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page83_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page85_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page86_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page87_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page89_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page8_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page8_2.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page90_1.csv'\r\n",
      "'/model_pipeline/model_pipeline/data/extraction/NYSE_TOT_2015 annual_page9_1.csv'\r\n"
     ]
    }
   ],
   "source": [
    "!ls  $file_config.data_dir/extraction/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/19/2020 21:15:56 - INFO - model_pipeline.relevance_infer -   ###### Received 265 examples for Table, number of questions: 20\n",
      "08/19/2020 21:17:53 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
      "08/19/2020 21:17:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 5238-0\n",
      "Clear Text: \n",
      " \tpage: 274\n",
      " \tpdfname: NYSE_TOT_2015 annual\n",
      " \ttext: What is the target carbon reduction in percentage?\n",
      " \ttext_b: (M$), As of December 31, 2013, Proved properties . . . . . . . . . . . . . . . . . ., Unproved properties . . . . . . . . . . . . . . . . ., Total capitalized costs . . . . . . . . . . . . . ., Accumulated depreciation, depletion and, amortization . . . . . . . . . . . . . . . . . . . ., Net capitalized costs . . . . . . . . . . . . . . ., As of December 31, 2014, Proved properties . . . . . . . . . . . . . . . . . ., Unproved properties . . . . . . . . . . . . . . . . ., Total capitalized costs . . . . . . . . . . . . . ., Accumulated depreciation, depletion and, amortization . . . . . . . . . . . . . . . . . . . ., Net capitalized costs . . . . . . . . . . . . . . ., As of December 31, 2015, Proved properties . . . . . . . . . . . . . . . . . ., Unproved properties . . . . . . . . . . . . . . . . ., Total capitalized costs . . . . . . . . . . . . . ., Accumulated depreciation, depletion and, amortization . . . . . . . . . . . . . . . . . . . ., Net capitalized costs . . . . . . . . . . . . . . ., Europe, Africa, Americas, Middle, East, Asia — CIS, (excl. Russia), Russia, Total\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġtarget', 'Ġcarbon', 'Ġreduction', 'Ġin', 'Ġpercentage', '?']\n",
      " \ttokens_b: ['(', 'M', '$', '),', 'ĠAs', 'Ġof', 'ĠDecember', 'Ġ31', ',', 'Ġ2013', ',', 'ĠPro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠUn', 'pro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠTotal', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠAcc', 'um', 'ulated', 'Ġdepreciation', ',', 'Ġdepletion', 'Ġand', ',', 'Ġam', 'ort', 'ization', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠNet', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠAs', 'Ġof', 'ĠDecember', 'Ġ31', ',', 'Ġ2014', ',', 'ĠPro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠUn', 'pro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠTotal', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠAcc', 'um', 'ulated', 'Ġdepreciation', ',', 'Ġdepletion', 'Ġand', ',', 'Ġam', 'ort', 'ization', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'ĠNet', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 1002, 4363, 4878, 11, 3164, 116, 2, 2, 1640, 448, 1629, 238, 287, 9, 719, 1105, 6, 1014, 6, 1698, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 1890, 4892, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 5480, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 5438, 783, 12944, 15910, 6, 39309, 8, 6, 524, 2723, 1938, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 5008, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 287, 9, 719, 1105, 6, 777, 6, 1698, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 1890, 4892, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 5480, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 5438, 783, 12944, 15910, 6, 39309, 8, 6, 524, 2723, 1938, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 5008, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 2]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n",
      "08/19/2020 21:17:53 - INFO - farm.data_handler.processor -   \n",
      "\n",
      "      .--.        _____                       _      \n",
      "    .'_\\/_'.     / ____|                     | |     \n",
      "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
      "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
      "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
      "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
      "   (/\\||/                             |_|           \n",
      "______\\||/___________________________________________                     \n",
      "\n",
      "ID: 204-0\n",
      "Clear Text: \n",
      " \tpage: 275\n",
      " \tpdfname: NYSE_TOT_2015 annual\n",
      " \ttext: What is the company name?\n",
      " \ttext_b: s of December 31, 2013, roved properties . . . . . . . . . . . . . . . . . ., nproved properties . . . . . . . . . . . . . . . . ., otal capitalized costs . . . . . . . . . . . . . ., ccumulated depreciation, depletion and, amortization . . . . . . . . . . . . . . . . . . . ., et capitalized costs . . . . . . . . . . . . . . ., s of December 31, 2014, roved properties . . . . . . . . . . . . . . . . . ., nproved properties . . . . . . . . . . . . . . . . ., otal capitalized costs . . . . . . . . . . . . . ., ccumulated depreciation, depletion and, amortization . . . . . . . . . . . . . . . . . . . ., et capitalized costs . . . . . . . . . . . . . . ., s of December 31, 2015, roved properties . . . . . . . . . . . . . . . . . ., nproved properties . . . . . . . . . . . . . . . . ., otal capitalized costs . . . . . . . . . . . . . ., ccumulated depreciation, depletion and, amortization . . . . . . . . . . . . . . . . . . . ., et capitalized costs . . . . . . . . . . . . . . ., Revision of historical costs out of ASC932 perimeter., Europe, Africa, Americas, Middle, East, Asia — CIS, (excl. Russia)(a), Russia, Total\n",
      "Tokenized: \n",
      " \ttokens: ['What', 'Ġis', 'Ġthe', 'Ġcompany', 'Ġname', '?']\n",
      " \ttokens_b: ['s', 'Ġof', 'ĠDecember', 'Ġ31', ',', 'Ġ2013', ',', 'Ġro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġn', 'pro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġot', 'al', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġc', 'cum', 'ulated', 'Ġdepreciation', ',', 'Ġdepletion', 'Ġand', ',', 'Ġam', 'ort', 'ization', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġet', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġs', 'Ġof', 'ĠDecember', 'Ġ31', ',', 'Ġ2014', ',', 'Ġro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġn', 'pro', 'ved', 'Ġproperties', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġot', 'al', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġc', 'cum', 'ulated', 'Ġdepreciation', ',', 'Ġdepletion', 'Ġand', ',', 'Ġam', 'ort', 'ization', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',', 'Ġet', 'Ġcapital', 'ized', 'Ġcosts', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ.', ',']\n",
      "Features: \n",
      " \tinput_ids: [0, 2264, 16, 5, 138, 766, 116, 2, 2, 29, 9, 719, 1105, 6, 1014, 6, 4533, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 295, 4892, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 19313, 337, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 740, 30639, 12944, 15910, 6, 39309, 8, 6, 524, 2723, 1938, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 4400, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 579, 9, 719, 1105, 6, 777, 6, 4533, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 295, 4892, 5202, 3611, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 19313, 337, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 740, 30639, 12944, 15910, 6, 39309, 8, 6, 524, 2723, 1938, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 4400, 812, 1538, 1042, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 479, 6, 2]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "_____________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 332/332 [00:41<00:00,  7.97 Batches/s]\n",
      "08/19/2020 21:18:34 - INFO - model_pipeline.relevance_infer -   Saved 262 relevant examples for Table in /model_pipeline/model_pipeline/data/infer/Table.csv\n"
     ]
    }
   ],
   "source": [
    "component.run_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls $infer_config.result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(os.path.join(infer_config.result_dir, \"Table.csv\")).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting statistics for train and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_set = \"val\"\n",
    "file = \"../model_pipeline/data/{}_split_02.csv\".format(data_set)\n",
    "test_data = pd.read_csv(file, index_col=0)\n",
    "\n",
    "from farm.infer import Inferencer\n",
    "model = Inferencer.load(\"../model_pipeline/saved_models/test_farm/\")\n",
    "result = model.inference_from_file(file)\n",
    "results = [d for r in result for d in r[\"predictions\"]]\n",
    "preds = [int(r[\"label\"]) for r in results]\n",
    "test_data[\"pred\"] = preds\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, recall_score, precision_score, f1_score, accuracy_score\n",
    "groups = test_data.groupby(\"text\")\n",
    "scores = {}\n",
    "for group, data in groups:\n",
    "    pred = data.pred\n",
    "    true = data.label\n",
    "    scores[group] = {}\n",
    "    scores[group][\"accuracy\"] = accuracy_score(true,pred)\n",
    "    scores[group][\"f1_score\"] = f1_score(true, pred)\n",
    "    scores[group][\"recall_score\"] = recall_score(true, pred)\n",
    "    scores[group][\"precision_score\"] = precision_score(true, pred)\n",
    "    scores[group][\"support\"] = len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df = pd.DataFrame(scores).to_csv(\"../model_pipeline/data/{}_table_metric.csv\".format(data_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
